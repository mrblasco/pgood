
19-Oct-2016

RE:  MS-16-01986, "Contributing to Public Goods Inside Organizations: Field Experimental Evidence"

DECISION: Reject

FROM:  Prof. John List, Department Editor, Management Science


Dear authors,

I have received one review and an Associate Editor (AE) report on your paper.  Unfortunately, both are negative.  The AE provides a thoughtful note on why s/he opted to conclude the review process upon receiving one report.  In the end, I very much like this study, and believe that it will find a home in a very respectable journal.  I just could not muster enough enthusiasm to get it over the line at MS.  Many apologies for not doing better, but I must pass on your very interesting study.  I hope that this decision does not dissuade you from sending us your work in the future.

Best,
John

------------

For office use:

MS-16-01986;11-Oct-2016;List;John;University of Chicago;DE Decision;19-Oct-2016;24-Oct-2016;Reject

------------

Associate Editor's text comments (if any)

Associate Editor
Comments to Author:
I now have one report in from an expert reviewer on the paper ” Contributing to Public Goods Inside Organizations: Field Experimental Evidence”. I had sent the paper to other reviewers but this reviewer was so fast and thorough in his/her comments so that I after having gone through them realized that I actually had enough information to make a recommendation, and then I wanted to save the authors time by giving them a fast recommendation. Unfortunately, the recommendation is reject. The reviewer is concerned mainly with novelty, the design, and statistical power, but also raises a number of other concerns. Even though I find the topic of the paper very interesting, I agree with these concerns. I hope the authors will find the comments useful and constructive.

------------

Reviewers' text comments (if any)

Reviewer: 1

Comments to the Author
The authors report the findings from a field experiment where employees of a Cardiology center at MGH could submit ideas for different types of improvements to their area. The treatments concerned the type of incentives or “cues” that the employees received in their solicitations: a “material”, individual incentive in one treatment, an appeal to either the improvement of patient care or the workplace in two other treatments, and a “control” condition that mad more salient how much money the unit would invest in the “winning” proposal.

The results indicate that the individual reward (an iPad) attracted more proposals, but that these proposals were not, on average, of higher quality than the proposals coming from subjects in the other three treatments. The authors infer that these employees responded to multiple motivations in their decision to provide to an “organizational public good”.

My feedback is reported in the points that follow.


NOVELTY AND CONTRIBUTION

The authors mention that there are several other lab and field experiments on the subjects of the provision of ideas in organizations and how to motivate this provision, and on the role of social preferences in the workplace. In particular they refer to recent work such as Della Vigna et al. (2016) and Gibbs et al. (2015). I would like to see further discussion on how the current study differs from these and others to which the paper refers. Gibbs et al. is particularly close, of course. Is the main difference that the current study is not in a for-profit firm? Or that the focus is less on customers and more on the internal organization?  Why should we expect these differences to lead to different results and new insights?


PARTICIPATION RATES

Participation rates were rather small. This creates some “technical” problems in terms of statistical power and ability, for example, in exploring heterogeneous effects to understand mechanisms. More importantly, this raises questions as to how employees perceived this call for submission; one possibility is that they just did not take it seriously, and only a small, maybe peculiar set of people decided to participate. Additional investigations (both quantitative and qualitative/anecdotal) may help understand whether there was sufficient attention to this call to warrant its analysis. Was this something people talked about at the center? Were there any initiatives to enhance the salience of this competition?
Again, my main concern here is whether we are looking at something that was quite marginal.


THE TREATMENTS AND THEIR INTERPRETATION

Some of the treatments may have different interpretations.

For example, was it clear that the iPad was to reward the winning contribution(s) as opposed to be a lottery reward to be randomly given to one or more of the participants? I received a few times invitations to take part to surveys, in my workplace, with an iPad to be assigned with a lottery to one of the participants. This may explain the success in stimulating participation (there is some “fun” utility in participating to a lottery), but the null effect on performance.

Also, the conditions appealing to improvements in the workplace or improvements of patient care may “nudge” the type of ideas that the promoter of the initiatives might expect to receive.

Finally, is the “fund” condition a sort of “control” case with the difference that the funding amount is made explicit? In other words, was the same funding amount present in the other treatments, just not mentioned? May the employees have assumed that there was a funding amount, thus attenuating the response in this treatment? May the amount itself, and not the features of the treatment, discourage participation somehow (maybe perceived as a waste of resources, or conversely too small to produce any relevant change?) Would the omission of the funding amount in the other treatments represent some form of “deception” to the subjects? And, finally, of course one might object that the experiment does not have a pure “baseline” or control condition against which to compare the others.


PERFORMANCE ANALYSIS

The analysis of the ratings shows some larger differences for top rates (5), with the prize treatment performing worse at these highest levels (a smaller share of high rate proposals). The small sample size here makes it difficult to make inferences and conclusions, but maybe this tells something about what types of individuals (and their ideas) the individual reward attracted? A few more words of what the objective of the initiative was in the first place might also help figuring out what are the outcomes of greater interest.

A further measure of quality or performance, only briefly mentioned by the authors, concerns the share of first stage proposals that made it to the second stage (was there an established average proportion set to be moved to the next stage? Were the participants aware of their odds of moving forward?). Here again we deal with small numbers, but it seems that the success rate was about 56% for the prize and workplace treatment, 50% for the fund treatment, and 43% of the pcare treatment. Are these differences large enough to be worthy more discussion, or maybe the authors are concerned that the small numbers don’t allow for much of a conclusion on this?