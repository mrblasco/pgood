18-Aug-2017

Dear Dr. Blasco:

I write you in regards to manuscript # AEJApp-2017-0200 entitled "Incentives for Public Goods Inside Organizations: Field Experimental Evidence" which you submitted to the American Economic Journal: Applied Economics.  Please find my decision letter attached, as well as two referee reports below.

Sincerely,
Prof. Dave Donaldson
Coeditor, American Economic Journal: Applied Economics

Reviewer(s)' Comments to Author:
Referee: 1

Comments to the Author
Referee report for ‘Incentives for Public Goods inside Organisations: Field Experimental Evidence’

This is a field experiment, in the context of a hospital that wants to induce proposals to improve the workplace. A contest was announced to solicit project proposals from employees. These projects, if implemented, could make the workplace better for everybody, hence the public good nature of the innovation. The treatments take the form of personalised messages to workers. The messages could emphasise different aspects of the contest: the existence of a prize, the resources devoted to the potential implementation of the project, the opportunity to improve the healthcare of the patients, and the opportunity to improve the workplace. 
There are lots of results, with all kinds of interactions being analysed. In addition, the left hand side variables can be the submission of a proposal, the quality of the proposal and the number of proposals submitted. Summarising, and hoping to be fair to the paper, I would emphasise the following:
•	The likelihood of submitting a proposal is higher in the prize treatment, and then in the opportunity to improve treatments, relative to the fund treatment. 
•	The higher number of proposals submitted is not at the expense of lower quality.
•	The treatment effects are enormous relative to reasonable measures of the cost of participating, so in addition to the value of the prize, there must be some type of underlying preferences towards the public good or mission-oriented preferences.
I don’t have particularly deep comments. The experiment seems well implemented and the conclusions are reasonable. I am not particularly exited given that there seems to be a lot of work on providing incentives to workers. Three aspects are unusual: the contest element, the innovation element, and the public good element. In addition, I like the fact that this is in the field. The stakes are presumably high and the workers are in an unusual environment. Extrapolating from here to other settings, especially settings where we may want to extract ideas from workers, seems very natural. 
•	As the authors mention, this is an issue that has received a lot of attention in applied work. On the one hand, we know that if we pay workers to do something, they will do it, so the finding that having a prize encourages the submission of proposals is not very surprising. Arguably, there are differences between this setting and other settings (the ‘innovation’ aspect of the product demanded from workers, the fact that workers may value directly this product which turns the production technology into a setting with public good/freeriding elements). However, are these differences significant enough such that we would expect monetary incentives to have no effect on effort? 
•	The other finding is that emphasising the opportunity to improve the workplace/customer care also encourages more effort than the fund treatment. Again, we know from other work that framing can have significant effects as long as these are short-lived and do not require an enormous amount of effort. Submitting a proposal in less than 300 words would probably qualify here. Therefore, I don’t find this effect particularly surprising either. 
•	Not having a control group makes the interpretation of the effects much more complicated than it had to be. Would it have been so difficult to write ‘Submit your ideas to the Ether Dome Challenge’? By writing this, they could have essentially not added anything to the information that comes in the paragraph immediately after. 
•	Unless I missed it, the paper contains no measure of the benefits of implementing the winning project. This is calibrated based on estimates from other parts, but it would be great to have some measure of how much the organisation benefitted, net of implementation costs, from creating this contest and implementing the winning project. 
•	The paper doesn’t seem to incorporate the notion that the value of implementing a project will be higher when the idea behind that project is better, and that this could be potentially observed by the workers. The first conclusion from there is that emphasising that the project will be implemented at a high cost (20,000) could even discourage proposals from workers who do not want the responsibility of having their mediocre ideas turn into an expensive disasters. 
•	The second conclusion is that all the demographic effects and the interactions become very difficult to interpret. Men and women may on average occupy different positions in the organisation, which provides them with different opportunities to obtain high-quality ideas. While the paper interprets any gender and other differences in terms of costs or preferences, it may just be that some workers just don’t have the capacity to submit proposals at the relatively low cost required by the contest. Consider, for instance, the patient care treatment. Imagine that women work with patients and men work in the back office. The treatment is emphasising that the contest will prioritise ideas related to patient care. Women have more ideas related to patient care. Hence, women react more to this treatment (‘controlling for profession’ is obviously insufficient here given how coarse the control is).  


Referee: 2

Comments to the Author
“Incentives for Public Goods Inside OrganizationsL Field Experimental Evidence” presents the results of a field experiment in which over 1200 members of the “Heart Center,” which is part of Massachusetts General Hospital, were invited to submit ideas for improvements for the center as part of an innovation contest. The invitations were made in one of four treatments: (1) the FUND treatment, which indicated that up to $20k could be allocated to implement a winning idea; (2) the PRIZE treatment, which indicated that the person who submitted a winning idea would win an iPad mini; (3) the PCARE treatment, which emphasized the value to patients of submitting ideas; and (4) the WPLACE treatment, which emphasized the value to the workplace of submitting ideas. Individuals were assigned to one of these four treatments and received three emails with the same treatment over the four weeks of the contest. Submission entailed writing an idea for improvement in less than 300 words. Results suggest that the PRIZE treatment generated significantly more individuals submitting ideas than the FUND treatment and significantly more ideas being submitted overall for the PRIZE treatment than the FUND treatment. Additional results include: no gender difference in submission but a difference in submission between PCARE and WPLACE by gender, no trade-off between quantity and quality of submission as based on both peer ratings and ratings of hospital staff, among other findings. The paper also presents a model of submission and attempts to parameterize it using data from the experiment.   

What motivates workers to contribute to public goods in the workplace is an important topic that indeed deserves attention. The authors investigate a particularly interesting setting (i.e., a medical center at a hospital) and observe contributions that may indeed generate a benefit to the workers and the workplace overall. Nevertheless, the paper has some limitations and room for improvement, which I describe below. I start with my more major concerns and list more minor issues below that. 

First, as the authors point out, the experiment lacks a control condition and so the authors can only compare between experimental treatments. This is a much bigger problem than is presented in the paper and makes the results of the paper very hard to interpret. In footnote 10, the authors posit that the FUND treatment is the closest thing to a control condition. They write: “Nevertheless, if we were to think of one treatment as the benchmark against which to compare the others, the FUND treatment would be our best candidate because giving information about the size of available funding is the default option for announcing grant programs and was part of the HTL’s initial design before our cooperation in the experiment.” However, the FUND treatment is not a control condition. A real control condition would provide no additional information relative to the other treatments (e.g. the control would be the baseline and additional information would be added to the other treatments), which would allow for a clean interpretation of the effect of the additional information provided beyond the control. In this case, the FUND treatment notes “win project funding up to $20,000 to turn your ideas into actions” while the other treatments replace this line with other motivations to submit. Unfortunately, there are reasons to believe that the FUND treatment would have a negative effect on submission in this context. A negative effect could arise because the “win project funding up to $20,000 to turn your ideas into actions” seems like it might inform subjects (or imply to subjects) that: (a) ideas must be implementable for $20k or less (thereby discouraging ideas that might be more expensive and/or forcing subjects to think harder about the cost of the idea), and (b) submissions commit the subject to be involved in implementation. It is easy to see why both of these could have negative effects on submission. Subjects who have ideas that are costly or who have ideas but do not want to be on the hook for implementing them may refrain from submitting in the FUND treatment. Consequently, comparing the other treatments to the FUND treatment necessarily conflates a potentially negative effect of the FUND treatment with whatever effect is induced by the other treatments (e.g. the explicit mentioning of the iPad mini prize in the PRIZE treatment). This is particularly rough for the paper given that the only statistically significant difference the authors find on submission is between the FUND treatment and the PRIZE treatment (they also find a marginally significant difference between FUND and WPLACE treatments). Given that it is hard to tell what is driving the difference in submission across these tretments (is the effect of PRIZE positive or is it negative and just less negative than FUND), it is hard to learn generalizable lessons from the experiment. (We indeed learn that PRIZE is preferable to FUND, but we don’t know why.) 

Second, the authors spend a good fraction of the paper analyzing the 113 responses collected across the 4 treatments to evaluate their relative quality both by peer raters and by HTL staff. While it is nice that the data is available to do this, the initial (relatively) small sample size combined with the low take-up rate makes these comparisons under-powered to find real differences between the treatment (this is exacerbated when using responses from the HTL staff who throw out an additional 20 responses for not meeting basic criteria). It is not surprising that it is hard to rule out treatment differences in quality when there are only ~25 or so observations in each treatment (indeed a proper analysis of these comparisons should also cluster for subject who submits them, which will further decrease power). This critique generates significant concerns about the null results found in sections 6.2 through 6.4. 

Third, the authors posit a model of the contributions in this setting in Section III and then attempt to parameterize it it section 6.5. Unfortunately, I do not think the model adds anything to the paper (I would advise the authors to cut it entirely). The authors start section 3 by stating: “In this section, we conceptualize an internal solicitation for innovation project proposals to improve the operations of the organization as a voluntary contribution mechanism for a public good.” However, the authors make a number of  restrictive simplifying assumptions and the model delivers comparative statics that are straightforward and therefore either ex post obvious, essentially identical to models of tournament entry in other contexts, or both. As for the simplifying assumptions: the assumption that Y is the sum of all submitted x’s seems immediately contradictory to the setting insofar as only one (or a handful) of ideas will be implemented in a tournament like this one. So if x is a random variable of idea quality, Y should be something like the max of all submitted x’s. Similarly, the probability of winning should be the probability that your x is equal to this max. In a more realistic model like this, all of a sudden you should start thinking about confidence in your idea or allow the costs of idea to be increasing in its quality. As for the comparative statics, it seems obvious that increasing the financial incentive or the value of the public good will increase contribution.  As for the calibration exercise in 6.5, it tries to identify how much subjects care about the public good created by idea generation, but the assumptions here again strain reality. (The authors highlight a few limitations in the last paragraph of 6.5 but there are many more.) In addition to assuming the model from section III is accurate, the calibration assumes that the treatments only affect behavior through the financial incentive and that agents have homogenous $40 costs of idea submission and particular beliefs about the number of submissions (the authors call these “rational expectations” but they choose numbers that are well above what the actual submission rate actually is, which confused me, and in their Figure 8 exercise they do not simulate using the actual number of submissions). Finally, even if the calibration exercise did not have these problems, it is unclear to me why the calibration would be worth doing, since it is unlikely this estimate would have any external validity outside of the study.

Finally, I have a number of other questions and concerns:
- Abstract and elsewhere: the authors say “Offering a prize for winning submissions boosted participation by 85 percent without affecting the quality of the submissions.” What this 85% increase is over (I presume the average of the other treatments) is unclear. 
- Footnote 2: Why cite Levitt and List (2007) for evidence of laboratory evidence on public good games when it is explicitly a critique of the laboratory method? There are chapters on laboratory public good games in the two Handbooks for Experimental Economics as well as other overview papers on the topic.   
- P3: The authors write “Analysis of the peer ratings indicates that there was no crowding-out effect. The higher propensity of participation for employees in the PRIZE treatment does not seem to be driven by low-quality submissions.” This is not exactly what the literature generally means by a “crowding-out” effect. While the result suggests no quantity-quality tradeoff, “crowding-out” usually refers to the possibility of an extrinsic incentive decreasing intrinsic motivation (e.g. if the iPad mini prize offer were to generate fewer contributions than a treatment that did not mention the prize).  
- p18 and p27: “mildly significant” is not a thing, say “marginally statistically significant”.
- p18 and elsewhere: I do not think that comparing the submission rates of men and women is a fair test of the gender and competitiveness hypotheses of Niederle and Vesterlund (2007) since there may be many reasons in this setting why men and women might respond differently in their submissions. A better test would be to compare men and women who are invited to contribute ideas in a non-competitive setting to different men and women who are invited to contribute ideas in a tournament and to look for a difference-in-difference. Obviously that cannot be done here, so I would cut the gender analysis.
- p20 and Table 5: I did not understand the regression specification here. The table notes say “(there is no specific reference category)” but I do not see WPLACExmale in the table. I also do not know why we care about all these interactions compared to men in the WPLACE treatment. 
- p21: the gender difference between PCARE and WPLACE might satisfy the kind of difference-in-difference test for men and women that I describe above, and maybe it is a real finding, but given the 4 treatments (and thus all the possible comparisons), I worry about multiple hypothesis testing in this context.
- p21: In 6.2 the authors first report that they find no effect of treatment on volunteering to evaluate proposals and write: “Thus, and overall, our data indicate
no prolonged effects of the treatments on both the extensive and intensive margin. This result is consistent with the general propensity of the effects from nudging and framing interventions to vanish over time.” But it is unclear to me why we would expect to find a result on evaluating submissions based on the treatment, which was about encouraging submission.
- Figure 6: I think this figure would be much clearer as a CDF with 4 lines, one for each treatment, and score (1-5) on the X axis.
- p25: The HTL staff threw out 20 proposals. The authors note these are not correlated with treatment, but they should redo the earlier analysis on the effect of treatment on submission excluding these 20 to see if anything changes (since even in the 20 are not significantly correlated with treatment, even a mild correlation could affect the significance of the original tests).